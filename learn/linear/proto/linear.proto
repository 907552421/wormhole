package dmlc.linear;
import "../../../repo/ps-lite/src/proto/filter.proto";

// configuration for linear method
message Config {

  // input & output

  optional string train_data = 0;
  optional string val_data = 2;

  optional string model_in = 3;
  optional string model_out = 4;

  // objective function

  enum Loss {
    SQUARE = 1;
    LOGIT = 2;
    HINGE = 3;
    SQUARE_HINGE = 4;
  }
  optional Loss loss = 10;

  enum Penalty {
    L1 = 1; // lambda(0) * ||w||_1 + .5 * lambda(1) * ||w||_F^2
    L2 = 2;  // .5 * lambda(0) * ||w||_F^2
  }
  optional Penalty penalty = 11;
  repeated float lambda = 12;

  // optimization methods

  enum Algo {
    // (minibatch) online methods
    SGD = 1;  // standard sgd
    ADAGRAD = 2;  // adaptive gradient descent
    FTRL = 3;

    // batch methdos
    LBFGS = 4;

    // stochastic (block) coordinate descent
    DARLIN = 5;
  }
  optional Algo algo = 20;

  // the size of minibatch for online methods
  optional int32 minibatch = 21 [default = 10000];

  // the maximal number of data passes
  optional int32 max_data_pass = 22 [default = 10];


  // convergance critiria. stop if the relative objective <= epsilon
  optional double epsilon = 24 [default = 1e-4];

  // learning rate
  enum LearningRate {
    CONSTANT = 1; // = lr_eta
    DECAY = 2;    // = lr_eta / (lr_beta + x), where x is user-defined, such as
                  // sqrt(iter), or the cumulative gradient on adagrad
  }
  optional LearningRate lr = 30;
  optional float lr_eta = 31;
  optional float lr_beta = 32;

  // to reduce sync cost

  // maximal allowed delay during synchronization
  optional int32 max_delay = 40 [default = 0];

  // features which occurs <= *tail_feature_freq* will be filtered. it save both
  // memory and bandwidth.
  optional int32 tail_feature_freq = 41 [default = 0];

  // filters to reduce communication costs
  repeated FilterConfig push_filter = 42;
  repeated FilterConfig pull_filter = 43;

  // method specific options

  Darlin darlin = 50;
}

message Darlin {
  // Divide a feature group into feature_block_ratio x nnz_feature_per_example
  // blocks
  optional float feature_block_ratio = 1 [default = 4];
  // Use a random order to updating feature block.  Turn it off to elimiate the
  // randomness (often for debuging), but may affects the convergence rate.
  optional bool random_feature_block_order = 2 [default = true];
  // Updating important feature group at the beginning to get a good initial
  // start point.
  repeated int32 prior_fea_group = 14;
  optional int32 num_iter_for_prior_fea_group = 13 [default = 5];
  optional int32 max_num_parallel_groups_in_preprocessing = 9 [default = 1000];
  // Used by the trust region method. All changes of parameters will be bounded
  // by *delta*. *delta* is updated according to the convergence,  whose intial
  // value is *delta_init_value* and maximal value is *delta_max_value*
  optional double delta_init_value = 101 [default = 1];
  optional double delta_max_value = 102 [default = 5];
  // kkt_filter_threshold = max_gradient_violation / num_examples *
  // kkt_filter_threshold_ratio. increasing this number reduces the effect of
  // kkt filter.
  optional double kkt_filter_threshold_ratio = 103 [default = 10];
}
